{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kB3PCDF5IiF4",
        "outputId": "24b2aa07-6b73-40ed-9979-0778b1bbcfa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autogen-agentchat~=0.2\n",
            "  Downloading autogen_agentchat-0.2.36-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting diskcache (from autogen-agentchat~=0.2)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting docker (from autogen-agentchat~=0.2)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting flaml (from autogen-agentchat~=0.2)\n",
            "  Downloading FLAML-2.3.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from autogen-agentchat~=0.2) (1.26.4)\n",
            "Collecting openai>=1.3 (from autogen-agentchat~=0.2)\n",
            "  Downloading openai-1.51.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from autogen-agentchat~=0.2) (24.1)\n",
            "Requirement already satisfied: pydantic!=2.6.0,<3,>=1.10 in /usr/local/lib/python3.10/dist-packages (from autogen-agentchat~=0.2) (2.9.2)\n",
            "Collecting python-dotenv (from autogen-agentchat~=0.2)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from autogen-agentchat~=0.2) (2.4.0)\n",
            "Collecting tiktoken (from autogen-agentchat~=0.2)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->autogen-agentchat~=0.2) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.3->autogen-agentchat~=0.2) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai>=1.3->autogen-agentchat~=0.2)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai>=1.3->autogen-agentchat~=0.2)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->autogen-agentchat~=0.2) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->autogen-agentchat~=0.2) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->autogen-agentchat~=0.2) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.6.0,<3,>=1.10->autogen-agentchat~=0.2) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.6.0,<3,>=1.10->autogen-agentchat~=0.2) (2.23.4)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from docker->autogen-agentchat~=0.2) (2.32.3)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from docker->autogen-agentchat~=0.2) (2.2.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->autogen-agentchat~=0.2) (2024.9.11)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.3->autogen-agentchat~=0.2) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.3->autogen-agentchat~=0.2) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.3->autogen-agentchat~=0.2) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai>=1.3->autogen-agentchat~=0.2)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.3->autogen-agentchat~=0.2)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->docker->autogen-agentchat~=0.2) (3.3.2)\n",
            "Downloading autogen_agentchat-0.2.36-py3-none-any.whl (370 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m370.6/370.6 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.51.0-py3-none-any.whl (383 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.5/383.5 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading FLAML-2.3.1-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.3/313.3 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-dotenv, jiter, h11, flaml, diskcache, tiktoken, httpcore, docker, httpx, openai, autogen-agentchat\n",
            "Successfully installed autogen-agentchat-0.2.36 diskcache-5.6.3 docker-7.1.0 flaml-2.3.1 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 jiter-0.5.0 openai-1.51.0 python-dotenv-1.0.1 tiktoken-0.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install autogen-agentchat~=0.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter Your OpenAI API Key:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tV6h5v5AJIX0",
        "outputId": "22692d05-8333-4ab5-9ce1-bb0eeb813ee8"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Your OpenAI API Key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from autogen import ConversableAgent\n",
        "\n",
        "agent = ConversableAgent(\n",
        "    \"chatbot\",\n",
        "    llm_config={\"config_list\": [{\"model\": \"gpt-4\", \"api_key\": os.environ.get(\"OPENAI_API_KEY\")}]},\n",
        "    code_execution_config=False,  # Turn off code execution, by default it is off.\n",
        "    function_map=None,  # No registered functions, by default it is None.\n",
        "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
        ")"
      ],
      "metadata": {
        "id": "3qND1ktfIxfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reply = agent.generate_reply(messages=[{\"content\": \"Tell me a joke.\", \"role\": \"user\"}])\n",
        "print(reply)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bh0qtVmvI-Wq",
        "outputId": "d1b543ed-a239-4657-bbe2-d7f4b2b9e4e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure, here's a classic one for you:\n",
            "\n",
            "Why don't scientists trust atoms?\n",
            "\n",
            "Because they make up everything!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cathy = ConversableAgent(\n",
        "    \"cathy\",\n",
        "    system_message=\"Your name is Cathy and you are a part of a duo of comedians.\",\n",
        "    llm_config={\"config_list\": [{\"model\": \"gpt-4\", \"temperature\": 0.9, \"api_key\": os.environ.get(\"OPENAI_API_KEY\")}]},\n",
        "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
        ")\n",
        "\n",
        "joe = ConversableAgent(\n",
        "    \"joe\",\n",
        "    system_message=\"Your name is Joe and you are a part of a duo of comedians.\",\n",
        "    llm_config={\"config_list\": [{\"model\": \"gpt-4\", \"temperature\": 0.7, \"api_key\": os.environ.get(\"OPENAI_API_KEY\")}]},\n",
        "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
        ")"
      ],
      "metadata": {
        "id": "-kWozxhhJhy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = joe.initiate_chat(cathy, message=\"Cathy, tell me a joke.\", max_turns=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pktNNh6wJjXq",
        "outputId": "c555cba0-e11a-452a-acc0-5ffd2bf7d07e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "joe (to cathy):\n",
            "\n",
            "Cathy, tell me a joke.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "cathy (to joe):\n",
            "\n",
            "Sure, here is a classic one for you:\n",
            "\n",
            "Why don't scientists trust atoms?\n",
            "\n",
            "Because they make up everything!\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "joe (to cathy):\n",
            "\n",
            "Hahaha, that's a good one Cathy! Okay, I've got a joke for you too. \n",
            "\n",
            "Why don't we ever tell secrets on a farm?\n",
            "\n",
            "Because the potatoes have eyes, the corn has ears, and the beans stalk!\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "cathy (to joe):\n",
            "\n",
            "Hahaha, that's a great one, Joe! You're giving me a run for my money in this comedy duo.\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}